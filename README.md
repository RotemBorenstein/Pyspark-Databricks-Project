# Pyspark Databricks Data Analysis Project

This repository contains two projects. Each project involves working with **PySpark** in a **Databricks environment** to process and analyze large-scale datasets.


## Project 1: Data Processing and Distributed Storage

**Objective:**

- Analyze and clean a dataset containing media viewing information.
- Detect and remove spam records based on predefined conditions.
- Design an optimized data distribution strategy for efficient retrieval.

**Technologies Used:**

- PySpark
- DataFrames and SQL in Spark
- Parquet file storage
- CSV data processing


## Project 2: Recommender System for TV Channels

**Objective:**

- Preprocess and normalize demographic and viewing data.
- Apply dimensionality reduction techniques (SVD & PCA) for analysis.
- Perform K-Means clustering to group households based on viewing habits.
- Implement a real-time Spark Streaming pipeline to analyze live data.

**Technologies Used:**

- PySpark MLLib (SVD, PCA, K-Means)
- Spark Streaming with Kafka
- Matplotlib for visualization


## License

This project is for educational purposes as part of the **Distributed Database Management** course at the Technion.

