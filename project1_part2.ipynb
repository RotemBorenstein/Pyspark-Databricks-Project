{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RotemBorenstein/Pyspark-Databricks-Project/blob/main/project1_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "326520e8-a13c-4688-9d5b-cf1fb2b96266",
          "showTitle": false,
          "title": ""
        },
        "id": "mMyyajyuAnTB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import split, array_contains, col, expr, to_date, dayofweek, when, countDistinct, desc, count, avg, lower, explode, max, collect_list, lit\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession, Row\n",
        "import re\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"my_project_part_2\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2a72b00c-0062-4ca3-a6bd-f6781710c11b",
          "showTitle": false,
          "title": ""
        },
        "id": "AbeJMFniAnTE"
      },
      "outputs": [],
      "source": [
        "# Read a CSV into a dataframe\n",
        "# There is a smarter version, that will first check if there is a Parquet file and use it\n",
        "def load_csv_file(filename, schema):\n",
        "  # Reads the relevant file from distributed file system using the given schema\n",
        "\n",
        "  allowed_files = {'Daily program data': ('Daily program data', \"|\"),\n",
        "                   'demographic': ('demographic', \"|\")}\n",
        "\n",
        "  if filename not in allowed_files.keys():\n",
        "    print(f'You were trying to access unknown file \\\"{filename}\\\". Only valid options are {allowed_files.keys()}')\n",
        "    return None\n",
        "\n",
        "  filepath = allowed_files[filename][0]\n",
        "  dataPath = f\"dbfs:/mnt/coursedata2024/fwm-stb-data/{filepath}\"\n",
        "  delimiter = allowed_files[filename][1]\n",
        "\n",
        "  df = spark.read.format(\"csv\")\\\n",
        "    .option(\"header\",\"false\")\\\n",
        "    .option(\"delimiter\",delimiter)\\\n",
        "    .schema(schema)\\\n",
        "    .load(dataPath)\n",
        "  return df\n",
        "\n",
        "# This dict holds the correct schemata for easily loading the CSVs\n",
        "schemas_dict = {'Daily program data':\n",
        "                  StructType([\n",
        "                    StructField('prog_code', StringType()),\n",
        "                    StructField('title', StringType()),\n",
        "                    StructField('genre', StringType()),\n",
        "                    StructField('air_date', StringType()),\n",
        "                    StructField('air_time', StringType()),\n",
        "                    StructField('Duration', FloatType())\n",
        "                  ]),\n",
        "                'viewing':\n",
        "                  StructType([\n",
        "                    StructField('device_id', StringType()),\n",
        "                    StructField('event_date', StringType()),\n",
        "                    StructField('event_time', IntegerType()),\n",
        "                    StructField('mso_code', StringType()),\n",
        "                    StructField('prog_code', StringType()),\n",
        "                    StructField('station_num', StringType())\n",
        "                  ]),\n",
        "                'viewing_full':\n",
        "                  StructType([\n",
        "                    StructField('mso_code', StringType()),\n",
        "                    StructField('device_id', StringType()),\n",
        "                    StructField('event_date', IntegerType()),\n",
        "                    StructField('event_time', IntegerType()),\n",
        "                    StructField('station_num', StringType()),\n",
        "                    StructField('prog_code', StringType())\n",
        "                  ]),\n",
        "                'demographic':\n",
        "                  StructType([StructField('household_id',StringType()),\n",
        "                    StructField('household_size',IntegerType()),\n",
        "                    StructField('num_adults',IntegerType()),\n",
        "                    StructField('num_generations',IntegerType()),\n",
        "                    StructField('adult_range',StringType()),\n",
        "                    StructField('marital_status',StringType()),\n",
        "                    StructField('race_code',StringType()),\n",
        "                    StructField('presence_children',StringType()),\n",
        "                    StructField('num_children',IntegerType()),\n",
        "                    StructField('age_children',StringType()), #format like range - 'bitwise'\n",
        "                    StructField('age_range_children',StringType()),\n",
        "                    StructField('dwelling_type',StringType()),\n",
        "                    StructField('home_owner_status',StringType()),\n",
        "                    StructField('length_residence',IntegerType()),\n",
        "                    StructField('home_market_value',StringType()),\n",
        "                    StructField('num_vehicles',IntegerType()),\n",
        "                    StructField('vehicle_make',StringType()),\n",
        "                    StructField('vehicle_model',StringType()),\n",
        "                    StructField('vehicle_year',IntegerType()),\n",
        "                    StructField('net_worth',IntegerType()),\n",
        "                    StructField('income',StringType()),\n",
        "                    StructField('gender_individual',StringType()),\n",
        "                    StructField('age_individual',IntegerType()),\n",
        "                    StructField('education_highest',StringType()),\n",
        "                    StructField('occupation_highest',StringType()),\n",
        "                    StructField('education_1',StringType()),\n",
        "                    StructField('occupation_1',StringType()),\n",
        "                    StructField('age_2',IntegerType()),\n",
        "                    StructField('education_2',StringType()),\n",
        "                    StructField('occupation_2',StringType()),\n",
        "                    StructField('age_3',IntegerType()),\n",
        "                    StructField('education_3',StringType()),\n",
        "                    StructField('occupation_3',StringType()),\n",
        "                    StructField('age_4',IntegerType()),\n",
        "                    StructField('education_4',StringType()),\n",
        "                    StructField('occupation_4',StringType()),\n",
        "                    StructField('age_5',IntegerType()),\n",
        "                    StructField('education_5',StringType()),\n",
        "                    StructField('occupation_5',StringType()),\n",
        "                    StructField('polit_party_regist',StringType()),\n",
        "                    StructField('polit_party_input',StringType()),\n",
        "                    StructField('household_clusters',StringType()),\n",
        "                    StructField('insurance_groups',StringType()),\n",
        "                    StructField('financial_groups',StringType()),\n",
        "                    StructField('green_living',StringType())\n",
        "                  ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d29452b3-ac5d-461a-8c58-1cddad8bb900",
          "showTitle": false,
          "title": ""
        },
        "id": "snV6f9u4AnTG"
      },
      "outputs": [],
      "source": [
        "# load Demographic Data\n",
        "demo_df = load_csv_file('demographic', schemas_dict['demographic'])\n",
        "\n",
        "# convert letters to appropriate numbers\n",
        "mapping = {'A' : 10, 'B': 11, 'C': 12, 'D': 13}\n",
        "demo_df = demo_df.withColumn(\"income\",\n",
        "            when(col(\"income\") == \"A\", mapping['A']).\n",
        "             when(col(\"income\") == \"B\", mapping['B']).\n",
        "             when(col(\"income\") == \"C\", mapping['C']).\n",
        "             when(col(\"income\") == \"D\", mapping['D']).\n",
        "             otherwise(col(\"income\")))\n",
        "demo_df = demo_df.withColumn(\"income\", col(\"income\").cast(\"int\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "418c0729-cdc6-47bf-bb54-52e7f3f1a1ea",
          "showTitle": false,
          "title": ""
        },
        "id": "OtNHEssMAnTG"
      },
      "outputs": [],
      "source": [
        "# load Daily program data\n",
        "daily_prog_df = load_csv_file('Daily program data', schemas_dict['Daily program data'])\n",
        "\n",
        "# convert genre to array of strings\n",
        "daily_prog_df = daily_prog_df.withColumn(\"genre\", split(col(\"genre\"), \",\\s*\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1417062a-b31e-4941-9fc4-4c299f7d0eac",
          "showTitle": false,
          "title": ""
        },
        "id": "Bvx1ujjRAnTH"
      },
      "outputs": [],
      "source": [
        "# load program viewing data\n",
        "dataPath = f\"dbfs:/viewing_10M\"\n",
        "viewing10m_df = spark.read.format(\"csv\")\\\n",
        "    .option(\"header\",\"true\")\\\n",
        "    .option(\"delimiter\",\",\")\\\n",
        "    .schema(schemas_dict['viewing_full'])\\\n",
        "    .load(dataPath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8081706c-8a86-40dd-adee-059fe913fe5b",
          "showTitle": false,
          "title": ""
        },
        "id": "-cHT29LaAnTH"
      },
      "outputs": [],
      "source": [
        "#load refrence data\n",
        "ref_df = spark.read.parquet('dbfs:/refxml_new_parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e807504a-1dd2-4b14-b5d9-4a4ee840c16f",
          "showTitle": false,
          "title": ""
        },
        "id": "kyP5v0HoAnTI"
      },
      "source": [
        "### part B.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ed1033fb-502a-472f-b0fc-f11d24563098",
          "showTitle": false,
          "title": ""
        },
        "id": "0n2BAEQAAnTJ"
      },
      "outputs": [],
      "source": [
        "# clean data\n",
        "daily_prog_df_cleaned_B1 = daily_prog_df.select(\"genre\", \"prog_code\").dropna(subset=[\"genre\", \"prog_code\"]).dropDuplicates(subset=[\"genre\", \"prog_code\"])\n",
        "ref_df_cleaned_B1 = ref_df.select(\"device_id\", \"dma\").dropna(subset=[\"device_id\", \"dma\"]).dropDuplicates(subset=[\"device_id\", \"dma\"])\n",
        "viewing10m_df_cleaned_B1 = viewing10m_df.select(\"device_id\", \"prog_code\").dropna(subset=[\"device_id\", \"prog_code\"]).dropDuplicates(subset=[\"device_id\", \"prog_code\"])\n",
        "\n",
        "# explode the 'genre' column\n",
        "exploded_daily_prog_df = daily_prog_df_cleaned_B1.withColumn(\"genre\", explode(\"genre\"))\n",
        "\n",
        "# join and select necessary columns\n",
        "ref_df_cleaned_B1_filtered = ref_df_cleaned_B1.filter(col(\"dma\") != \"Unknown\")\n",
        "ref_and_viewing = ref_df_cleaned_B1_filtered.join(viewing10m_df_cleaned_B1, on=\"device_id\").select(\"device_id\", \"prog_code\", \"dma\")\n",
        "ref_viewing_daily = ref_and_viewing.join(exploded_daily_prog_df, on=\"prog_code\").select(\"device_id\", \"prog_code\", \"dma\", \"genre\")\n",
        "\n",
        "# cache ref_viewing_daily because we will reused multiple times\n",
        "ref_viewing_daily.cache()\n",
        "\n",
        "# get the top 10 DMAs by device amount\n",
        "largest_dma = ref_df_cleaned_B1_filtered.groupBy(\"dma\").agg(count(\"device_id\").alias(\"device_amount\")).orderBy(col(\"device_amount\").desc()).limit(10).select(\"dma\")\n",
        "\n",
        "# filter ref_viewing_daily to only include records with DMAs in largest_dma\n",
        "ref_viewing_daily_largest_dma = ref_viewing_daily.join(largest_dma, on=\"dma\")\n",
        "\n",
        "# calculate the viewing entries per genre for each DMA\n",
        "dma_populatity = ref_viewing_daily_largest_dma.groupBy(\"dma\", \"genre\").agg(count(\"prog_code\").alias(\"viewing_entries\")).orderBy(col(\"viewing_entries\").desc())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d9ae9293-0798-4d10-adce-0ce6f19d0a40",
          "showTitle": false,
          "title": ""
        },
        "id": "4khh7_edAnTJ",
        "outputId": "a48a172a-77b7-4b5a-9485-4f28d3887d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved DataFrame for DMA: Charleston-Huntington to project1_part21_CharlestonHuntington_314689498_211620570.csv\nSaved DataFrame for DMA: Wilkes Barre-Scranton-Hztn to project1_part21_Wilkes_BarreScrantonHztn_314689498_211620570.csv\nSaved DataFrame for DMA: Seattle-Tacoma to project1_part21_SeattleTacoma_314689498_211620570.csv\nSaved DataFrame for DMA: Toledo to project1_part21_Toledo_314689498_211620570.csv\nSaved DataFrame for DMA: Little Rock-Pine Bluff to project1_part21_Little_RockPine_Bluff_314689498_211620570.csv\nSaved DataFrame for DMA: Amarillo to project1_part21_Amarillo_314689498_211620570.csv\nSaved DataFrame for DMA: Bend, OR to project1_part21_Bend_OR_314689498_211620570.csv\nSaved DataFrame for DMA: Greenville-N.Bern-Washngtn to project1_part21_GreenvilleNBernWashngtn_314689498_211620570.csv\nSaved DataFrame for DMA: Washington, DC (Hagrstwn) to project1_part21_Washington_DC_Hagrstwn_314689498_211620570.csv\nSaved DataFrame for DMA: Houston to project1_part21_Houston_314689498_211620570.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[device_id: string, prog_code: string, dma: string, genre: string]"
            ]
          },
          "execution_count": 16,
          "metadata": {}
        }
      ],
      "source": [
        "# get distinct DMAs\n",
        "distinct_dmas = largest_dma.distinct().collect()\n",
        "dma_list = [row[\"dma\"] for row in distinct_dmas]\n",
        "\n",
        "def clean_dma_name(dma):\n",
        "    clean_dma = re.sub(r\"\\s\", \"_\", dma)\n",
        "    return re.sub(r'[^A-Za-z0-9_]', '', clean_dma.replace(' ', '_'))\n",
        "\n",
        "# filter the DataFrame for each DMA and save to CSV\n",
        "for dma in dma_list:\n",
        "    cleaned_dma = clean_dma_name(dma)\n",
        "    dma_filtered_df = dma_populatity.filter(dma_populatity[\"dma\"] == dma)\n",
        "    file_name = f\"project1_part21_{cleaned_dma}_314689498_211620570.csv\"\n",
        "    dma_filtered_df.write.csv(\n",
        "        path= file_name,\n",
        "        mode=\"overwrite\",\n",
        "        header=True\n",
        "    )\n",
        "    print(f\"Saved DataFrame for DMA: {dma} to {file_name}\")\n",
        "\n",
        "# uncache ref_viewing_daily\n",
        "ref_viewing_daily.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2e6ee963-9c37-4789-bd1c-b20a7ed3d294",
          "showTitle": false,
          "title": ""
        },
        "id": "3jmQ05abAnTL"
      },
      "outputs": [],
      "source": [
        "dma_populatity.cache()\n",
        "\n",
        "# Function to display the top 10 genres for a specific DMA\n",
        "def display_top_genres_for_dma(dma, dma_populatity, title):\n",
        "    print(f\"# {title} - {dma}\")\n",
        "    top_genres = dma_populatity.filter(col(\"dma\") == dma).orderBy(col(\"viewing_entries\").desc()).limit(10).select(\"genre\")\n",
        "    top_genres.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e4d1f822-fac2-4a60-94e6-84bcc7048c78",
          "showTitle": false,
          "title": ""
        },
        "id": "nKD5zbPqAnTM",
        "outputId": "4ead48e1-4b8b-4042-8e40-9411d41d8837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Top 10 Genres for the 1st Largest DMA - Charleston-Huntington\n+-----------+\n|genre      |\n+-----------+\n|Reality    |\n|News       |\n|Sitcom     |\n|Drama      |\n|Talk       |\n|Documentary|\n|Children   |\n|Adventure  |\n|Comedy     |\n|Animated   |\n+-----------+\n\n"
          ]
        }
      ],
      "source": [
        "# Display top 10 genres for the 1st, 5th, and 9th DMAs by size\n",
        "display_top_genres_for_dma(dma_list[0], dma_populatity, \"Top 10 Genres for the 1st Largest DMA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dab62d5f-a6a4-4599-96ad-8b0bcb97afdd",
          "showTitle": false,
          "title": ""
        },
        "id": "BeIyRV8KAnTM",
        "outputId": "f5a27b5d-5612-4f96-ea0c-194ffa761adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Top 10 Genres for the 5th Largest DMA - Little Rock-Pine Bluff\n+-----------+\n|genre      |\n+-----------+\n|Reality    |\n|News       |\n|Sitcom     |\n|Talk       |\n|Drama      |\n|Comedy     |\n|Documentary|\n|Crime drama|\n|Adventure  |\n|Children   |\n+-----------+\n\n"
          ]
        }
      ],
      "source": [
        "display_top_genres_for_dma(dma_list[4], dma_populatity, \"Top 10 Genres for the 5th Largest DMA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1e172af2-4143-4800-94e0-ec9ebf2edbd8",
          "showTitle": false,
          "title": ""
        },
        "id": "3A1wNt27AnTN",
        "outputId": "e282df5c-73ae-4a99-f666-dc063b665230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Top 10 Genres for the 9th Largest DMA - Washington, DC (Hagrstwn)\n+-----------+\n|genre      |\n+-----------+\n|Reality    |\n|News       |\n|Sitcom     |\n|Comedy     |\n|Children   |\n|Drama      |\n|Talk       |\n|Animated   |\n|Crime drama|\n|Adventure  |\n+-----------+\n\n"
          ]
        }
      ],
      "source": [
        "display_top_genres_for_dma(dma_list[8], dma_populatity, \"Top 10 Genres for the 9th Largest DMA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "61474333-d349-4385-9ffb-9a58357e654a",
          "showTitle": false,
          "title": ""
        },
        "id": "AZw31fcnAnTN",
        "outputId": "b8d9424e-9b56-4b54-c9bf-710a6eecf90d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[dma: string, genre: string, viewing_entries: bigint]"
            ]
          },
          "execution_count": 21,
          "metadata": {}
        }
      ],
      "source": [
        "dma_populatity.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "917ad6a1-ae45-46b3-8551-af681296123e",
          "showTitle": false,
          "title": ""
        },
        "id": "ggkBWXvPAnTO"
      },
      "source": [
        "### part B.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "660f12ef-902c-4e8a-8d6b-ebe963c81800",
          "showTitle": false,
          "title": ""
        },
        "id": "AtFz1isyAnTO"
      },
      "outputs": [],
      "source": [
        "# clean data\n",
        "demo_df_cleaned_B2 = demo_df.dropna(\n",
        "    subset=[\"income\", \"net_worth\", \"household_id\"]\n",
        ").dropDuplicates(subset=[\"income\", \"net_worth\", \"household_id\"])\n",
        "\n",
        "daily_prog_df_cleaned_B2 = daily_prog_df.select(\"genre\", \"prog_code\").dropna(\n",
        "    subset=[\"genre\", \"prog_code\"]\n",
        ").dropDuplicates(subset=[\"genre\", \"prog_code\"])\n",
        "\n",
        "ref_df_cleaned_B2 = ref_df.select(\"device_id\", \"household_id\", \"dma\").dropna(\n",
        "    subset=[\"device_id\", \"household_id\", \"dma\"]\n",
        ").dropDuplicates(subset=[\"device_id\", \"dma\"])\n",
        "\n",
        "viewing10m_df_cleaned_B2 = viewing10m_df.select(\"device_id\", \"prog_code\").dropna(\n",
        "    subset=[\"device_id\", \"prog_code\"]\n",
        ").dropDuplicates(subset=[\"device_id\", \"prog_code\"])\n",
        "\n",
        "# explode the 'genre' column\n",
        "daily_prog_df_cleaned_B2 = daily_prog_df_cleaned_B2.withColumn(\"genre\", explode(\"genre\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8cd740bd-75cf-4181-8768-aa0c3e208191",
          "showTitle": false,
          "title": ""
        },
        "id": "6HNZ4FJtAnTO"
      },
      "outputs": [],
      "source": [
        "# calculate wealth score for each DMA\n",
        "max_income = demo_df_cleaned_B2.agg(max(\"income\")).collect()[0][0]\n",
        "max_net_worth = demo_df_cleaned_B2.agg(max(\"net_worth\")).collect()[0][0]\n",
        "\n",
        "dma_wealth_score = (\n",
        "    demo_df_cleaned_B2\n",
        "    .join(ref_df_cleaned_B2, on=\"household_id\")\n",
        "    .groupBy(\"dma\")\n",
        "    .agg(\n",
        "        avg(\"net_worth\").alias(\"avg_net_worth\"),\n",
        "        avg(\"income\").alias(\"avg_income\")\n",
        "    )\n",
        "    .withColumn(\"wealth_score\", (col(\"avg_net_worth\") / max_net_worth) + (col(\"avg_income\") / max_income))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ec70172-9d2f-42cb-860f-51c066eb1baf",
          "showTitle": false,
          "title": ""
        },
        "id": "MfDrj2XXAnTP"
      },
      "outputs": [],
      "source": [
        "# get 10 Wealthiest DMA\n",
        "top_wealthy_dmas = dma_wealth_score.orderBy(desc(\"wealth_score\")).limit(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a5242a59-898b-44f0-93ca-82a679d21026",
          "showTitle": false,
          "title": ""
        },
        "id": "ACZQIOJoAnTP",
        "outputId": "081835db-7642-4c7f-b0cd-c4325ee7b7be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Top 11 Genres for the 1th Wealthiest DMA: San Antonio (Wealth Score: 1.624) ###\n+------------+\n|genre       |\n+------------+\n|News        |\n|Weather     |\n|Sitcom      |\n|Talk        |\n|Drama       |\n|Newsmagazine|\n|Western     |\n|Comedy      |\n|Cooking     |\n|Reality     |\n|Auto        |\n+------------+\n\n### Top 11 Genres for the 5th Wealthiest DMA: Bend, OR (Wealth Score: 1.457) ###\n+--------------+\n|genre         |\n+--------------+\n|Outdoors      |\n|Bus./financial|\n|History       |\n|Science       |\n|How-to        |\n|Animals       |\n|Nature        |\n|Medical       |\n|Golf          |\n|Playoff sports|\n|Paranormal    |\n+--------------+\n\n### Top 11 Genres for the 9th Wealthiest DMA: Seattle-Tacoma (Wealth Score: 1.416) ###\n+----------------+\n|genre           |\n+----------------+\n|Pro wrestling   |\n|Martial arts    |\n|Action sports   |\n|Parenting       |\n|Poker           |\n|Card games      |\n|Aviation        |\n|Military        |\n|Self improvement|\n|Anime           |\n|Softball        |\n+----------------+\n\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[dma: string, genre: string, viewing_count: bigint]"
            ]
          },
          "execution_count": 28,
          "metadata": {}
        }
      ],
      "source": [
        "# join the viewing data with the program data to get the genres\n",
        "viewing_with_genres = viewing10m_df_cleaned_B2.join(daily_prog_df_cleaned_B2, on=\"prog_code\")\n",
        "\n",
        "# count the occurrences of each genre within each DMA\n",
        "genre_popularity = (\n",
        "    viewing_with_genres\n",
        "    .join(ref_df_cleaned_B2, on=\"device_id\")\n",
        "    .groupBy(\"dma\", \"genre\")\n",
        "    .agg(count(\"prog_code\").alias(\"viewing_count\"))\n",
        "    .orderBy(desc(\"viewing_count\"))\n",
        ")\n",
        "\n",
        "# create a list to hold the final results\n",
        "results = []\n",
        "\n",
        "# track used genres\n",
        "used_genres = set()\n",
        "\n",
        "# function to clean DMA name for file naming\n",
        "def clean_dma_name(dma_name):\n",
        "    return re.sub(r'[^A-Za-z0-9_]', '', dma_name.replace(' ', '_'))\n",
        "\n",
        "# cache genre_popularity because we will reused multiple times\n",
        "genre_popularity.cache()\n",
        "\n",
        "# process each DMA in the order of wealth\n",
        "for idx, dma_row in enumerate(top_wealthy_dmas.collect()):\n",
        "    dma = dma_row[\"dma\"]\n",
        "    wealth_score = dma_row[\"wealth_score\"]\n",
        "\n",
        "    # get the top genres for the current DMA excluding already used genres\n",
        "    top_genres = (\n",
        "        genre_popularity\n",
        "        .filter(col(\"dma\") == dma)\n",
        "        .filter(~col(\"genre\").isin(list(used_genres)))\n",
        "        .orderBy(desc(\"viewing_count\"))\n",
        "        .select(\"genre\", \"dma\", \"viewing_count\")\n",
        "        .limit(11)\n",
        "    )\n",
        "\n",
        "    if idx in [0, 4, 8]:\n",
        "        print(f\"### Top 11 Genres for the {idx + 1}th Wealthiest DMA: {dma} (Wealth Score: {round(wealth_score, 3)}) ###\")\n",
        "        top_genres.select(\"genre\").show(truncate=False)\n",
        "\n",
        "    top_genres_list = [row[\"genre\"] for row in top_genres.collect()]\n",
        "\n",
        "    # add the genres to the used set\n",
        "    used_genres.update(top_genres_list)\n",
        "\n",
        "    file_name = f\"project1_part22_{dma}_314689498_211620570.csv\"\n",
        "    single_dma_df = top_genres.select(\"dma\", \"genre\").join(top_wealthy_dmas.select(\"dma\", \"wealth_score\"), on=\"dma\")\n",
        "    single_dma_df.withColumnRenamed(\"dma\", \"DMA NAME\")\n",
        "    single_dma_df.withColumnRenamed(\"wealth_score\", \"WEALTH SCORE\")\n",
        "    single_dma_df.withColumnRenamed(\"genre\", \"ORDERED LIST OF GENRES\")\n",
        "    single_dma_df.write.csv(file_name, header=True, mode=\"overwrite\")\n",
        "\n",
        "genre_popularity.unpersist()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "project1_part2_314689498_211620570",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}